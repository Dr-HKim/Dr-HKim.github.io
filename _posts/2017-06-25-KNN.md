---
layout: post  
title: KNN (K-Nearest Neighbor)  
date: 2017-06-25  
tags:   
published: true  
---

(작성 중)

KNN 은 매우 간단한 분류classification 방법론입니다. 

A 그룹과 B 그룹으로 분류된 데이터가 있습니다. 새로운 데이터가 관측되었을 때, 그리고 그 데이터가 어느 그룹에 속하는지 알 수 없을 때 이를 추정하기 위한 방법론입니다. 

통계적 방법론 가운데 K 가 앞에 나오는 것들은 대부분 사전에 정의된 K (pre-defined K) 를 씁니다. (K-means, K-익명성 등)

- 먼저 새로운 데이터와 기존 데이터 간의 거리를 측정합니다. 
- 가장 가까운 K 개의 샘플을 뽑습니다. K-Nearest Neighbor)
- K 개 샘플 가운데 더 많이 관찰된 그룹으로 새로운 데이터를 분류합니다.
- K = 5 이고 가장 가까운 5개 관측치 가운데 A 그룹이 3개, B 그룹이 2개 있다면 A그룹일 확률이 60% 가 됩니다. 확률이 가장 높은 그룹으로  분류합니다.

몇 가지 논의가 남아있습니다. 

- Outlier 는 제외합니다. 
- 두 관측치 사이의 거리를 구하는 방법은 여러가지입니다. 

위 알고리즘을 수식으로 표현하면 아래와 같습니다. 

(eq 2.12):

$$\Pr \left( Y = j | X = x_0 \right) = \frac{1}{K} \Sigma_{i \in N_o} I \left( y_i = j \right)$$


두 관측치 사이의 거리를 구하는 방법은 여러가지입니다. 

Euclidean Distance  
가장 기본적인 거리 계산법입니다. 

Manhattn Distance  
빌딩 사이를 피해간다고 생각하고 모든 x들 사이의 차이와 모든 y들 사이의 차이를 더하는 방법입니다. 


